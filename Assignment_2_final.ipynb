{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jathu03/Assignment-02/blob/main/Assignment_2_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vqrig87j2bW",
        "outputId": "4908cc64-7019-4756-8e1e-f07929a19975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[16.98566131  3.20993322 10.70589783  2.60898703  4.76761628  1.37645994]\n",
            " [ 8.22973974  4.83917687 15.98532335  9.01443039  3.9736362   1.89733064]\n",
            " [ 8.42291213  0.33576711  3.53614334  2.12159843  1.39766252  1.53552856]\n",
            " [ 2.32917512  5.51177081  4.29448351  4.68054086  1.23135024  5.70316062]\n",
            " [13.30410114  1.46427728  2.32975776  3.1703183   4.39376562  4.69652359]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def pairwise_squared_euclidean_distance(X, Y):\n",
        "    # ||x_i - y_j||^2 = ||x_i||^2 + ||y_j||^2 - 2 * <x_i,y_j>\n",
        "    # Computing the squared norms of matrices X and Y\n",
        "    X_norm_squared = np.sum(X**2, axis=1).reshape(-1, 1)  # getting norm of X as a column vector of shape =(m, 1)\n",
        "    Y_norm_squared = np.sum(Y**2, axis=1).reshape(1, -1)  # getting norm of Y as a row vector of shape = (1, n)\n",
        "\n",
        "    # Computing the dot product of X and Y\n",
        "    X_Y_dot_product = np.dot(X, Y.T)\n",
        "\n",
        "    return X_norm_squared + Y_norm_squared - 2 * X_Y_dot_product\n",
        "\n",
        "\n",
        "X = np.random.randn(5, 3)  # 5 x 3 matrix\n",
        "Y = np.random.randn(6, 3)  # 6 x 3 matrix\n",
        "\n",
        "Z = pairwise_squared_euclidean_distance(X, Y)\n",
        "\n",
        "print(Z)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yksLxOcToVlu",
        "outputId": "7ad73614-b791-4ba7-bc7d-d72ac735f003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "lAIkEhj_oZ7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to load the dataset and extract feature embedding\n",
        "\n",
        "# Define the transformation to be applied to the images\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "dataset = ImageFolder(root='/content/drive/MyDrive/caltech101', transform=data_transform)\n",
        "\n",
        "# Split dataset randomly into training and testing\n",
        "train_size = int(2/3 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders for training and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Load Pre-trained ResNet-50 Model\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "resnet50 = torch.nn.Sequential(*list(resnet50.children())[:-1])  # Remove the last classification layer\n",
        "resnet50.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Extract the feature embeddings\n",
        "def extract_features(data_loader, model):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.view(outputs.size(0), -1)  # Flatten the output tensor\n",
        "            features.append(outputs.numpy())\n",
        "            labels.append(targets.numpy())\n",
        "\n",
        "    features = np.concatenate(features, axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "    return features, labels\n",
        "\n",
        "# Extract features from train and test datasets\n",
        "train_features, train_labels = extract_features(train_loader, resnet50)\n",
        "test_features, test_labels = extract_features(test_loader, resnet50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg6m0OyWocfm",
        "outputId": "27616c1c-4d25-4b8a-ea4c-3553e47b1020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 196MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  k-NN Classification (Question 3.a)\n",
        "def knn_predict(test_features, train_features, train_labels, k):\n",
        "    predictions = []\n",
        "\n",
        "    # Calculate the euclidean distances\n",
        "    distances = pairwise_squared_euclidean_distance(test_features, train_features)\n",
        "\n",
        "    # Iterate over each test point\n",
        "    for i in range(test_features.shape[0]):\n",
        "        # Get the indices of the k nearest neighbors (smallest distances)\n",
        "        k_nearest_indices = np.argsort(distances[i])[:k]\n",
        "\n",
        "        # Get the labels of the k nearest neighbors\n",
        "        k_nearest_labels = [train_labels[j] for j in k_nearest_indices]\n",
        "\n",
        "        # Perform majority vote\n",
        "        most_common_label = Counter(k_nearest_labels).most_common(1)[0][0]\n",
        "        predictions.append(most_common_label)\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "# k-NN Classification\n",
        "k = 5\n",
        "\n",
        "# Evaluate on Test Set\n",
        "test_predictions = knn_predict(test_features, train_features, train_labels, k)\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "\n",
        "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_5q3smwogDn",
        "outputId": "89f76f04-376b-499f-d77c-2a71771cdaa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Accuracy: 88.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train Linear Classifier (Question 3.b)\n",
        "\n",
        "# Convert features and labels into tensors\n",
        "train_features_tensor = torch.tensor(train_features, dtype=torch.float32)  # Removed .to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)  # Removed .to(device)\n",
        "\n",
        "# Define the linear classifier\n",
        "num_classes = len(dataset.classes)\n",
        "input_dim = train_features.shape[1]\n",
        "\n",
        "classifier = nn.Linear(input_dim, num_classes)  # Removed .to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Training for the linear classifier\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "num_samples = train_features_tensor.shape[0]\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Mini-batch training\n",
        "    permutation = torch.randperm(num_samples)\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_features = train_features_tensor[indices]\n",
        "        batch_labels = train_labels_tensor[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = classifier(batch_features)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Evaluate the Test Set\n",
        "classifier.eval()\n",
        "test_features_tensor = torch.tensor(test_features, dtype=torch.float32)  # Removed .to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_outputs = classifier(test_features_tensor)\n",
        "    _, test_predictions = torch.max(test_outputs, 1)\n",
        "\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions.numpy())  # No need for .cpu()\n",
        "print(f\"Test Set Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-YoThEeoiSJ",
        "outputId": "eceb365a-0b46-446a-9258-f7c4f3551bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 244.8940\n",
            "Epoch [2/20], Loss: 51.4168\n",
            "Epoch [3/20], Loss: 30.3171\n",
            "Epoch [4/20], Loss: 19.9645\n",
            "Epoch [5/20], Loss: 13.6421\n",
            "Epoch [6/20], Loss: 10.7997\n",
            "Epoch [7/20], Loss: 7.8402\n",
            "Epoch [8/20], Loss: 6.6773\n",
            "Epoch [9/20], Loss: 5.0535\n",
            "Epoch [10/20], Loss: 4.2201\n",
            "Epoch [11/20], Loss: 3.4698\n",
            "Epoch [12/20], Loss: 2.9287\n",
            "Epoch [13/20], Loss: 2.6041\n",
            "Epoch [14/20], Loss: 2.5710\n",
            "Epoch [15/20], Loss: 2.1450\n",
            "Epoch [16/20], Loss: 1.7471\n",
            "Epoch [17/20], Loss: 1.4299\n",
            "Epoch [18/20], Loss: 1.3995\n",
            "Epoch [19/20], Loss: 1.2267\n",
            "Epoch [20/20], Loss: 1.0437\n",
            "Test Set Accuracy: 92.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Load and Split the Caltech-101 Dataset with Data Augmentation\n",
        "data_transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "data_transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the dataset from the drive and split it into training and testing\n",
        "dataset = ImageFolder(root='/content/drive/MyDrive/caltech101', transform=data_transform_train)\n",
        "train_size = int(2 / 3 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Apply test transform to the test dataset\n",
        "test_dataset.dataset.transform = data_transform_test\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Load Pre-trained ResNet-50 Model and Modify for Fine-tuning\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "\n",
        "# Modify the fully connected layer to match the number of classes in Caltech-101\n",
        "num_classes = len(dataset.classes)\n",
        "resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resnet50.parameters(), lr=0.0001)  # Use a small learning rate for fine-tuning\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # Learning rate scheduler\n",
        "\n",
        "# Train the Network\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    resnet50.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = resnet50(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    scheduler.step()  # Adjust the learning rate\n",
        "    train_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "# Evaluate on the test Set\n",
        "resnet50.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = resnet50(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vg-KaOFokoB",
        "outputId": "71c8ec8f-1add-4a8c-a945-efa977c3963a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 296.9842, Train Accuracy: 68.96%\n",
            "Epoch [2/10], Loss: 43.8542, Train Accuracy: 95.62%\n",
            "Epoch [3/10], Loss: 15.2190, Train Accuracy: 98.59%\n",
            "Epoch [4/10], Loss: 5.9009, Train Accuracy: 99.46%\n",
            "Epoch [5/10], Loss: 6.9574, Train Accuracy: 99.38%\n",
            "Epoch [6/10], Loss: 7.3933, Train Accuracy: 99.28%\n",
            "Epoch [7/10], Loss: 14.3735, Train Accuracy: 98.29%\n",
            "Epoch [8/10], Loss: 7.0534, Train Accuracy: 99.31%\n",
            "Epoch [9/10], Loss: 2.5168, Train Accuracy: 99.85%\n",
            "Epoch [10/10], Loss: 1.6772, Train Accuracy: 99.92%\n",
            "Test Set Accuracy: 95.05%\n"
          ]
        }
      ]
    }
  ]
}