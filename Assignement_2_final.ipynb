{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN7HYHDIDwxQUT0E5CRpuJF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jathu03/Assignment-02/blob/main/Assignement_2_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riNqYlmpgG1",
        "outputId": "a32406c5-8010-48b9-9fc2-63cb1d1c3f17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.02069749  6.77852076  4.15663733  6.43700597  4.88184863  6.62187736]\n",
            " [ 9.96512272  8.59341755  2.20121959 14.94321186  4.39381488 12.1101634 ]\n",
            " [ 0.2578211   7.62028864  8.95066564  3.90943138  7.23967365  4.00427178]\n",
            " [ 7.25978437  0.30973006  9.17800063  3.16384683  0.12724513  3.35101603]\n",
            " [ 5.39087401  3.39382984  3.45189363  6.54164954  1.1676055   3.0312488 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def pairwise_squared_euclidean_distance(X, Y):\n",
        "    # ||x_i - y_j||^2 = ||x_i||^2 + ||y_j||^2 - 2 * <x_i,y_j>\n",
        "    # Computing the squared norms of matrices X and Y\n",
        "    X_norm_squared = np.sum(X**2, axis=1).reshape(-1, 1)  # getting norm of X as a column vector of shape =(m, 1)\n",
        "    Y_norm_squared = np.sum(Y**2, axis=1).reshape(1, -1)  # getting norm of Y as a row vector of shape = (1, n)\n",
        "\n",
        "    # Computing the dot product of X and Y\n",
        "    X_Y_dot_product = np.dot(X, Y.T)\n",
        "\n",
        "    return X_norm_squared + Y_norm_squared - 2 * X_Y_dot_product\n",
        "\n",
        "\n",
        "X = np.random.randn(5, 3)  # 5 x 3 matrix\n",
        "Y = np.random.randn(6, 3)  # 6 x 3 matrix\n",
        "\n",
        "Z = pairwise_squared_euclidean_distance(X, Y)\n",
        "\n",
        "print(Z)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eyYrnphXqvG",
        "outputId": "27d7c832-f577-4b52-dad1-38ebd71993aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XL8oOEEc7YnG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "tQtX34JiuXpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to load the dataset and extract feature embedding\n",
        "\n",
        "# Define the transformation to be applied to the images\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "dataset = ImageFolder(root='/content/drive/MyDrive/caltech101', transform=data_transform)\n",
        "\n",
        "# Split dataset randomly into training and testing\n",
        "train_size = int(2/3 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders for training and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Load Pre-trained ResNet-50 Model\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "resnet50 = torch.nn.Sequential(*list(resnet50.children())[:-1])  # Remove the last classification layer\n",
        "resnet50.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Extract the feature embeddings\n",
        "def extract_features(data_loader, model):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.view(outputs.size(0), -1)  # Flatten the output tensor\n",
        "            features.append(outputs.numpy())\n",
        "            labels.append(targets.numpy())\n",
        "\n",
        "    features = np.concatenate(features, axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "    return features, labels\n",
        "\n",
        "# Extract features from train and test datasets\n",
        "train_features, train_labels = extract_features(train_loader, resnet50)\n",
        "test_features, test_labels = extract_features(test_loader, resnet50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGy8grVTXXBK",
        "outputId": "53e083c4-4d53-4b57-acf6-863ff8fc6752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 118MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  k-NN Classification (Question 3.a)\n",
        "def knn_predict(test_feature, train_features, train_labels, k):\n",
        "\n",
        "  distances = pairwise_squared_euclidean_distance(train_features, test_feature)  # Calculate Euclidean distances\n",
        "  k_nearest_indices = np.argsort(distances)[:k]  # Find k nearest neighbors\n",
        "\n",
        "  # Majority vote for the class label\n",
        "  class_counts = np.bincount(train_labels[k_nearest_indices])\n",
        "  return np.argmax(class_counts)\n",
        "\n",
        "\n",
        "# Evaluate on Test Set\n",
        "test_predictions = knn.predict(test_features)\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "\n",
        "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p23uwnZLpIDY",
        "outputId": "69a49296-a29f-42cf-86a9-6e32db53e922"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Accuracy: 90.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Linear Classifier (Question 3.b)\n",
        "\n",
        "# Convert features and labels into tensors\n",
        "train_features_tensor = torch.tensor(train_features, dtype=torch.float32).to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "\n",
        "# Define the linear classifier\n",
        "num_classes = len(dataset.classes)\n",
        "input_dim = train_features.shape[1]\n",
        "\n",
        "classifier = nn.Linear(input_dim, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Training for the linear classifier\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "num_samples = train_features_tensor.shape[0]\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Mini-batch training\n",
        "    permutation = torch.randperm(num_samples)\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_features = train_features_tensor[indices]\n",
        "        batch_labels = train_labels_tensor[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = classifier(batch_features)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Evaluate the Test Set\n",
        "classifier.eval()\n",
        "test_features_tensor = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_outputs = classifier(test_features_tensor)\n",
        "    _, test_predictions = torch.max(test_outputs, 1)\n",
        "\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions.cpu().numpy())\n",
        "print(f\"Test Set Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpmEL3Z-w7VN",
        "outputId": "2d1f5cdc-6f96-4196-c93b-ae3b95f5a35d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 248.7729\n",
            "Epoch [2/20], Loss: 51.8058\n",
            "Epoch [3/20], Loss: 31.4364\n",
            "Epoch [4/20], Loss: 19.5602\n",
            "Epoch [5/20], Loss: 14.1242\n",
            "Epoch [6/20], Loss: 10.2553\n",
            "Epoch [7/20], Loss: 8.0954\n",
            "Epoch [8/20], Loss: 5.8857\n",
            "Epoch [9/20], Loss: 5.0249\n",
            "Epoch [10/20], Loss: 4.3128\n",
            "Epoch [11/20], Loss: 3.3955\n",
            "Epoch [12/20], Loss: 3.2046\n",
            "Epoch [13/20], Loss: 2.8537\n",
            "Epoch [14/20], Loss: 2.2745\n",
            "Epoch [15/20], Loss: 2.3045\n",
            "Epoch [16/20], Loss: 1.8016\n",
            "Epoch [17/20], Loss: 1.7371\n",
            "Epoch [18/20], Loss: 1.5970\n",
            "Epoch [19/20], Loss: 1.4978\n",
            "Epoch [20/20], Loss: 1.2606\n",
            "Test Set Accuracy: 92.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Step 1: Load and Split the Caltech-101 Dataset with Data Augmentation\n",
        "# Data augmentation for better generalization\n",
        "data_transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "data_transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the entire dataset and split it into training and testing\n",
        "dataset = ImageFolder(root='/content/drive/MyDrive/caltech101', transform=data_transform_train)\n",
        "train_size = int(2 / 3 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Apply test transform to the test dataset\n",
        "test_dataset.dataset.transform = data_transform_test\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Step 2: Load Pre-trained ResNet-50 Model and Modify for Fine-tuning\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "\n",
        "# Modify the fully connected layer to match the number of classes in Caltech-101\n",
        "num_classes = len(dataset.classes)\n",
        "resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet50 = resnet50.to(device)\n",
        "\n",
        "# Step 3: Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resnet50.parameters(), lr=0.0001)  # Use a small learning rate for fine-tuning\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # Learning rate scheduler\n",
        "\n",
        "# Step 4: Train the Network\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    resnet50.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = resnet50(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    scheduler.step()  # Adjust the learning rate\n",
        "    train_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "# Step 5: Evaluate on Test Set\n",
        "resnet50.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = resnet50(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mCpE1i0yHC0",
        "outputId": "52ec0661-7bfe-4afd-8351-e7f62ad58ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 299.9618, Train Accuracy: 68.64%\n",
            "Epoch [2/10], Loss: 46.1127, Train Accuracy: 95.60%\n",
            "Epoch [3/10], Loss: 13.3897, Train Accuracy: 98.70%\n",
            "Epoch [4/10], Loss: 9.1128, Train Accuracy: 99.13%\n",
            "Epoch [5/10], Loss: 6.2251, Train Accuracy: 99.48%\n",
            "Epoch [6/10], Loss: 2.8371, Train Accuracy: 99.75%\n"
          ]
        }
      ]
    }
  ]
}